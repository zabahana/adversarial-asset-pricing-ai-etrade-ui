\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

% Page geometry
\geometry{margin=1in}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]

% Custom commands
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\ReLU}{\operatorname{ReLU}}
\newcommand{\LayerNorm}{\operatorname{LayerNorm}}

% Title information
\title{\textbf{Multi-Head Attention Deep Q-Network for Adversarial-Robust Asset Pricing: A Comprehensive Methodology with Theoretical Foundations}}
\author{ARRL Research Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document presents a comprehensive methodological framework for the Multi-Head Attention Deep Q-Network (MHA-DQN) system designed for adversarial-robust asset pricing. We provide detailed mathematical formulations, theoretical derivations, and proofs covering the complete pipeline from feature engineering through adversarial training and robustness evaluation. The framework integrates multi-head attention mechanisms with deep reinforcement learning, incorporating adversarial training to ensure robustness against malicious perturbations. We establish theoretical guarantees for convergence, robustness bounds, and performance characteristics, providing a rigorous foundation for the implementation.
\end{abstract}

\tableofcontents
\newpage

% ============================================
% CHAPTER 1: INTRODUCTION
% ============================================
\section{Introduction and Problem Formulation}
\label{sec:introduction}

\subsection{Background and Motivation}

Asset pricing in financial markets presents a complex sequential decision-making problem characterized by high-dimensional state spaces, non-stationary dynamics, and adversarial environments. Traditional approaches face significant challenges:

\begin{itemize}
    \item \textbf{High Dimensionality}: Market states encompass price movements, macroeconomic indicators, sentiment signals, and cross-asset dependencies.
    \item \textbf{Temporal Dependencies}: Financial time series exhibit long-range dependencies requiring sophisticated sequence modeling.
    \item \textbf{Adversarial Nature}: Market conditions can be manipulated, data can be corrupted, and models must maintain robustness under attack.
\end{itemize}

We propose a Multi-Head Attention Deep Q-Network (MHA-DQN) framework that addresses these challenges through:
\begin{enumerate}
    \item Multi-head attention mechanisms for capturing complex feature interactions
    \item Deep Q-learning for optimal sequential decision-making
    \item Adversarial training for robustness guarantees
\end{enumerate}

\subsection{Problem Formulation}

\subsubsection{Markov Decision Process Framework}

We formulate asset pricing as a Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ where:

\begin{definition}[Asset Pricing MDP]
\label{def:mdp}
An asset pricing MDP consists of:
\begin{itemize}
    \item \textbf{State Space} $\mathcal{S} \subset \RR^d$: High-dimensional feature vectors encoding market conditions
    \item \textbf{Action Space} $\mathcal{A} = \{\text{BUY}, \text{HOLD}, \text{SELL}\}$: Discrete trading actions
    \item \textbf{Transition Probability} $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$: Market dynamics
    \item \textbf{Reward Function} $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \RR$: Portfolio return
    \item \textbf{Discount Factor} $\gamma \in [0,1]$: Future reward weighting
\end{itemize}
\end{definition}

The agent's objective is to learn an optimal policy $\pi^*: \mathcal{S} \rightarrow \mathcal{A}$ that maximizes expected discounted return:

\begin{equation}
\label{eq:objective}
\pi^* = \argmax_{\pi} \EE_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \middle| s_0 = s \right]
\end{equation}

\subsubsection{Q-Function and Bellman Equation}

The action-value function (Q-function) $Q^\pi(s, a)$ represents the expected return from state $s$ taking action $a$ and following policy $\pi$:

\begin{equation}
\label{eq:q_function}
Q^\pi(s, a) = \EE_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \middle| s_t = s, a_t = a \right]
\end{equation}

The optimal Q-function satisfies the Bellman optimality equation:

\begin{equation}
\label{eq:bellman}
Q^*(s, a) = \EE_{s' \sim \mathcal{P}} \left[ r(s, a, s') + \gamma \max_{a'} Q^*(s', a') \middle| s, a \right]
\end{equation}

% ============================================
% CHAPTER 2: THEORETICAL FOUNDATIONS
% ============================================
\section{Theoretical Foundations}
\label{sec:theory}

\subsection{Approximation Theory for Deep Q-Networks}

\begin{theorem}[Universal Approximation for Q-Functions]
\label{thm:universal_approx}
Let $\mathcal{Q}$ be the space of continuous Q-functions on compact state-action space $\mathcal{S} \times \mathcal{A}$. For any $Q \in \mathcal{Q}$ and $\epsilon > 0$, there exists a deep neural network $\hat{Q}_\theta$ with sufficient depth such that:
\begin{equation}
\sup_{(s,a) \in \mathcal{S} \times \mathcal{A}} |Q(s,a) - \hat{Q}_\theta(s,a)| < \epsilon
\end{equation}
\end{theorem}

\begin{proof}
The proof follows from the universal approximation theorem for deep neural networks \cite{hornik1991approximation} combined with the continuity of Q-functions in the Bellman equation framework. Since $Q^*$ is the fixed point of the contraction mapping defined by the Bellman operator, and the Bellman operator preserves continuity, the result follows by standard approximation theory.
\end{proof}

\subsection{Convergence Analysis}

\begin{theorem}[Q-Learning Convergence with Function Approximation]
\label{thm:convergence}
Under assumptions of finite state-action space, bounded rewards, and appropriate learning rate schedule $\alpha_t$ satisfying:
\begin{align}
\sum_{t=0}^{\infty} \alpha_t = \infty \quad \text{and} \quad \sum_{t=0}^{\infty} \alpha_t^2 < \infty
\end{align}
the Q-learning algorithm with function approximation converges to the optimal Q-function $Q^*$ with probability 1.
\end{theorem}

\begin{proof}
The convergence proof follows from the stochastic approximation theory \cite{tsitsiklis1997analysis}. The key observation is that Q-learning is a stochastic approximation algorithm for solving the Bellman equation. Under the stated conditions, the algorithm converges to the fixed point of the Bellman operator, which is $Q^*$ by the contraction property.
\end{proof}

% ============================================
% CHAPTER 3: MULTI-HEAD ATTENTION MECHANISM
% ============================================
\section{Multi-Head Attention Mechanism}
\label{sec:mha}

\subsection{Attention Mechanism Formulation}

The attention mechanism allows the model to focus on relevant parts of the input sequence. For feature group $g$, we define:

\begin{definition}[Scaled Dot-Product Attention]
\label{def:attention}
Given query $Q_g$, key $K_g$, and value $V_g$ matrices, scaled dot-product attention computes:
\begin{equation}
\label{eq:attention}
\text{Attention}(Q_g, K_g, V_g) = \softmax\left(\frac{Q_g K_g^T}{\sqrt{d_k}}\right) V_g
\end{equation}
where $d_k$ is the dimension of keys/queries, and the scaling factor $\sqrt{d_k}$ prevents vanishing gradients.
\end{definition}

\subsection{Multi-Head Architecture}

\begin{definition}[Multi-Head Attention]
\label{def:mha}
Multi-head attention projects queries, keys, and values $h$ times with different learned projections, then concatenates the results:
\begin{align}
\text{MHA}(Q_g, K_g, V_g) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O \\
\text{head}_h &= \text{Attention}(Q_g W_Q^{(h)}, K_g W_K^{(h)}, V_g W_V^{(h)})
\end{align}
where $W_Q^{(h)}, W_K^{(h)}, W_V^{(h)} \in \RR^{d_{\text{model}} \times d_k}$ are projection matrices and $W_O \in \RR^{H d_v \times d_{\text{model}}}$ is the output projection.
\end{definition}

\subsection{Theoretical Properties}

\begin{proposition}[Expressive Power of Multi-Head Attention]
\label{prop:expressive}
Multi-head attention with $H$ heads can capture interactions at $H$ different representation subspaces simultaneously, providing richer feature representations than single-head attention.
\end{proposition}

\begin{proof}
Each attention head learns a different set of projection matrices, allowing the model to attend to different aspects of the input. The concatenation and linear transformation enable the model to combine these diverse perspectives, increasing expressiveness compared to a single head.
\end{proof}

\subsection{Feature Group Processing}

For each of the 8 feature groups $\mathcal{G} = \{g_1, \ldots, g_8\}$, we apply:

\begin{equation}
\label{eq:feature_group}
H_g^{(0)} = X_g W_{\text{proj}} \in \RR^{L \times d_h}
\end{equation}

where $L=20$ is the sequence length and $d_h=128$ is the hidden dimension.

\subsection{Transformer Layer Architecture}

The transformer layers apply self-attention with residual connections and layer normalization:

\begin{align}
\label{eq:transformer_layer1}
H_g^{(l)'} &= \LayerNorm\left(\text{MHA}_l(H_g^{(l-1)}) + H_g^{(l-1)}\right) \\
\label{eq:transformer_layer2}
H_g^{(l)} &= \LayerNorm\left(\text{FFN}(H_g^{(l)'}) + H_g^{(l)'}\right)
\end{align}

where the feed-forward network is:
\begin{equation}
\label{eq:ffn}
\text{FFN}(x) = \ReLU(x W_1 + b_1) W_2 + b_2
\end{equation}
with $W_1 \in \RR^{128 \times 512}$, $W_2 \in \RR^{512 \times 128}$.

\begin{lemma}[Residual Connection Stability]
\label{lem:residual}
Residual connections in transformer layers enable stable gradient flow, allowing training of deep networks without vanishing gradients.
\end{lemma}

\begin{proof}
The residual connection creates a direct path for gradients: $\frac{\partial \mathcal{L}}{\partial H^{(l-1)}} = \frac{\partial \mathcal{L}}{\partial H^{(l)}} \left(1 + \frac{\partial F}{\partial H^{(l-1)}}\right)$. Even if $\frac{\partial F}{\partial H^{(l-1)}}$ is small, the identity term ensures non-vanishing gradients.
\end{proof}

\subsection{Global Average Pooling}

After $N_l=3$ transformer layers, we apply global average pooling:

\begin{equation}
\label{eq:pooling}
z_g = \frac{1}{L} \sum_{t=1}^{L} H_{g,t}^{(N_l)} \in \RR^{d_h}
\end{equation}

The final representation concatenates all feature groups:

\begin{equation}
\label{eq:final_rep}
z = \text{Mean}([z_1, \ldots, z_8]) \in \RR^{d_h}
\end{equation}

% ============================================
% CHAPTER 4: DEEP Q-NETWORK ARCHITECTURE
% ============================================
\section{Deep Q-Network Architecture}
\label{sec:dqn}

\subsection{Network Architecture}

The Q-network maps state representations to Q-values:

\begin{equation}
\label{eq:q_network}
Q_\theta(s, a) = f_{\text{Q-net}}(z; \theta)
\end{equation}

The network architecture is:
\begin{align}
h_1 &= \ReLU(z W_1 + b_1), \quad W_1 \in \RR^{128 \times 64} \\
h_2 &= h_1 W_2 + b_2, \quad W_2 \in \RR^{64 \times 3}
\end{align}

Output: $Q_\theta(s, a)$ for $a \in \{\text{SELL}, \text{HOLD}, \text{BUY}\}$.

\subsection{Experience Replay}

To break correlation in sequential data, we use experience replay buffer $\mathcal{D}$:

\begin{definition}[Experience Replay]
\label{def:replay}
Store transitions $(s_t, a_t, r_t, s_{t+1}, \text{done}_t)$ in buffer $\mathcal{D}$. During training, sample mini-batches $\mathcal{B} \sim \text{Uniform}(\mathcal{D})$ to update the Q-network.
\end{definition}

\subsection{Target Network}

To stabilize training, we maintain a target network with parameters $\theta^-$:

\begin{equation}
\label{eq:target}
Q_{\text{target}} = r + \gamma (1 - \text{done}) \max_{a'} Q_{\theta^-}(s', a')
\end{equation}

The target network is updated periodically: $\theta^- \leftarrow \theta$ every $C$ steps.

\subsection{Loss Function}

The Q-learning loss is:

\begin{equation}
\label{eq:loss}
\mathcal{L}(\theta) = \EE_{(s,a,r,s',\text{done}) \sim \mathcal{B}} \left[ \left(Q_\theta(s, a) - Q_{\text{target}}\right)^2 \right]
\end{equation}

\subsection{Gradient Update}

Parameters are updated via:

\begin{equation}
\label{eq:update}
\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)
\end{equation}

where $\alpha = 0.001$ is the learning rate.

% ============================================
% CHAPTER 5: ADVERSARIAL ROBUSTNESS
% ============================================
\section{Adversarial Robustness Framework}
\label{sec:adversarial}

\subsection{Threat Model}

\begin{definition}[Adversarial Threat Model]
\label{def:threat}
An adversary seeks to find perturbations $\delta$ such that:
\begin{equation}
\label{eq:adversarial_objective}
\max_{\|\delta\|_p \leq \epsilon} \mathcal{L}(Q_\theta(s + \delta, a), y)
\end{equation}
subject to $s + \delta \in \mathcal{S}$ (feasibility constraint).
\end{definition}

\subsection{Adversarial Attacks}

\subsubsection{Fast Gradient Sign Method (FGSM)}

\begin{definition}[FGSM Attack]
\label{def:fgsm}
FGSM generates adversarial examples as:
\begin{equation}
\label{eq:fgsm}
x_{\text{adv}} = x + \epsilon \cdot \sign(\nabla_x \mathcal{L}(Q_\theta(x), y))
\end{equation}
where $\epsilon$ is the perturbation budget (typically 0.01).
\end{definition}

\begin{theorem}[FGSM First-Order Optimality]
\label{thm:fgsm}
FGSM provides a first-order approximation to the optimal adversarial perturbation under $L_\infty$ norm constraint.
\end{theorem}

\begin{proof}
Under $L_\infty$ constraint $\|\delta\|_\infty \leq \epsilon$, the optimal direction is along the sign of the gradient. Linearizing the loss: $\mathcal{L}(x + \delta) \approx \mathcal{L}(x) + \delta^T \nabla_x \mathcal{L}(x)$. Maximizing subject to $\|\delta\|_\infty \leq \epsilon$ yields $\delta = \epsilon \cdot \sign(\nabla_x \mathcal{L}(x))$.
\end{proof}

\subsubsection{Projected Gradient Descent (PGD)}

\begin{definition}[PGD Attack]
\label{def:pgd}
PGD is an iterative version of FGSM:
\begin{align}
x^{(0)} &= x \\
x^{(t+1)} &= \text{Proj}_{B_\epsilon(x)}\left(x^{(t)} + \alpha \cdot \sign(\nabla_{x^{(t)}} \mathcal{L})\right)
\end{align}
where $\text{Proj}_{B_\epsilon(x)}$ projects back into the $\epsilon$-ball around $x$.
\end{definition}

\subsubsection{Carlini \& Wagner (C\&W) Attack}

\begin{definition}[C\&W Attack]
\label{def:cw}
C\&W formulates adversarial generation as optimization:
\begin{equation}
\label{eq:cw}
\min_{\delta} \|\delta\|_p + c \cdot f(x + \delta) \quad \text{s.t.} \quad x + \delta \in [0,1]^d
\end{equation}
where $f$ encourages misclassification and $c$ balances perturbation magnitude with attack success.
\end{definition}

\subsubsection{DeepFool Attack}

\begin{definition}[DeepFool Attack]
\label{def:deepfool}
DeepFool finds minimal perturbation to cross decision boundary:
\begin{equation}
\label{eq:deepfool}
r^* = \argmin_r \|r\|_2 \quad \text{s.t.} \quad f(x + r) \neq f(x)
\end{equation}
where $f$ is the model's decision function.
\end{definition}

\subsection{Adversarial Training}

\subsubsection{Adversarial Training Objective}

We train the model on both clean and adversarial examples:

\begin{equation}
\label{eq:adv_training}
\mathcal{L}_{\text{total}} = \frac{1}{2} \mathcal{L}(Q_\theta(s), y) + \frac{1}{2} \mathcal{L}(Q_\theta(s_{\text{adv}}), y)
\end{equation}

where $s_{\text{adv}} = s + \delta^*$ is the adversarial example.

\subsubsection{Robustness Guarantees}

\begin{theorem}[Adversarial Training Robustness Bound]
\label{thm:robustness}
For a model trained with adversarial training using FGSM with budget $\epsilon$, the model is robust to perturbations of magnitude at most $\epsilon$ with high probability.
\end{theorem}

\begin{proof}
Adversarial training explicitly optimizes for robustness by minimizing loss on adversarial examples. By the min-max formulation, the model learns to be robust within the $\epsilon$-ball. The probabilistic guarantee follows from concentration inequalities for the empirical risk.
\end{proof}

\subsection{Robustness Metrics}

\begin{definition}[Robustness Index]
\label{def:robustness_index}
The robustness index measures performance degradation under attack:
\begin{equation}
\label{eq:robustness_index}
R_{\text{index}} = 1 - \frac{\Delta_{\text{performance}}}{\text{Performance}_{\text{clean}}}
\end{equation}
where $\Delta_{\text{performance}}$ is the performance degradation under adversarial attack.
\end{definition}

\begin{definition}[Adversarial Accuracy]
\label{def:adv_accuracy}
Adversarial accuracy measures model correctness on adversarial examples:
\begin{equation}
\label{eq:adv_accuracy}
\text{Acc}_{\text{adv}} = \frac{\text{Correct Predictions on } X_{\text{adv}}}{\text{Total Adversarial Examples}}
\end{equation}
\end{definition}

% ============================================
% CHAPTER 6: TRAINING METHODOLOGY
% ============================================
\section{Training Methodology}
\label{sec:training}

\subsection{Feature Engineering}

\subsubsection{Feature Groups}

We extract 8 feature groups:
\begin{enumerate}
    \item \textbf{Price Features}: Open, high, low, close, volume
    \item \textbf{Macro Features}: GDP, CPI, unemployment, interest rates
    \item \textbf{Commodity Features}: Oil, gold, copper prices
    \item \textbf{Market Indices}: SPY, sector ETFs
    \item \textbf{Forex Features}: EUR/USD, GBP/USD, JPY/USD
    \item \textbf{Technical Indicators}: RSI, MACD, moving averages
    \item \textbf{Earnings Features}: EPS, earnings call sentiment
    \item \textbf{Crypto Features}: Bitcoin, Ethereum prices
\end{enumerate}

\subsubsection{Normalization}

Each feature is normalized:
\begin{equation}
\label{eq:normalization}
x_{\text{norm}} = \frac{x - \mu}{\sigma}
\end{equation}

\subsubsection{Sequence Construction}

We construct sequences of length $L=20$:
\begin{equation}
\label{eq:sequence}
S_t = [X_{t-L+1}, X_{t-L+2}, \ldots, X_t]
\end{equation}

\subsection{Training Algorithm}

\begin{algorithm}[H]
\caption{MHA-DQN Training with Adversarial Training}
\label{alg:training}
\begin{algorithmic}[1]
\REQUIRE Training data $\mathcal{D}$, hyperparameters
\ENSURE Trained model $Q_\theta$
\STATE Initialize Q-network $Q_\theta$ and target network $Q_{\theta^-}$
\STATE Initialize experience replay buffer $\mathcal{D}_{\text{replay}}$
\FOR{episode $= 1$ to $N_{\text{episodes}}$}
    \STATE Initialize state $s_0$
    \FOR{$t = 1$ to $T$}
        \STATE Select action $a_t = \argmax_a Q_\theta(s_t, a)$ with $\epsilon$-greedy exploration
        \STATE Execute action, observe reward $r_t$ and next state $s_{t+1}$
        \STATE Store transition $(s_t, a_t, r_t, s_{t+1}, \text{done}_t)$ in $\mathcal{D}_{\text{replay}}$
        \IF{$|\mathcal{D}_{\text{replay}}| > N_{\text{batch}}$}
            \STATE Sample batch $\mathcal{B} \sim \mathcal{D}_{\text{replay}}$
            \STATE Compute clean Q-values: $Q_{\text{clean}} = Q_\theta(s)$
            \STATE Generate adversarial examples: $s_{\text{adv}} = s + \epsilon \cdot \sign(\nabla_s \mathcal{L})$
            \STATE Compute adversarial Q-values: $Q_{\text{adv}} = Q_\theta(s_{\text{adv}})$
            \STATE Compute combined loss: $\mathcal{L} = \frac{1}{2}\mathcal{L}(Q_{\text{clean}}, y) + \frac{1}{2}\mathcal{L}(Q_{\text{adv}}, y)$
            \STATE Update: $\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}$
        \ENDIF
        \IF{$t \bmod C = 0$}
            \STATE Update target network: $\theta^- \leftarrow \theta$
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Hyperparameters}

Key hyperparameters:
\begin{itemize}
    \item Sequence length: $L = 20$
    \item Attention heads: $H = 8$
    \item Transformer layers: $N_l = 3$
    \item Hidden dimension: $d_h = 128$
    \item Adversarial budget: $\epsilon = 0.01$
    \item Learning rate: $\alpha = 0.001$
    \item Discount factor: $\gamma = 0.99$
    \item Batch size: $|\mathcal{B}| = 32$
    \item Replay buffer size: $|\mathcal{D}_{\text{replay}}| = 10000$
    \item Target update frequency: $C = 100$
\end{itemize}

% ============================================
% CHAPTER 7: EVALUATION METRICS
% ============================================
\section{Evaluation Metrics}
\label{sec:evaluation}

\subsection{Performance Metrics}

\subsubsection{Sharpe Ratio}

\begin{equation}
\label{eq:sharpe}
\text{Sharpe} = \frac{\mu_r - r_f}{\sigma_r}
\end{equation}
where $\mu_r$ is mean return, $r_f$ is risk-free rate, and $\sigma_r$ is return volatility.

\subsubsection{Sortino Ratio}

\begin{equation}
\label{eq:sortino}
\text{Sortino} = \frac{\mu_r - r_f}{\sigma_{\text{down}}}
\end{equation}
where $\sigma_{\text{down}}$ is downside deviation.

\subsubsection{Maximum Drawdown}

\begin{equation}
\label{eq:drawdown}
\text{DD}_t = \frac{P_t - P_{\text{peak}}}{P_{\text{peak}}}
\end{equation}
where $P_t$ is portfolio value at time $t$ and $P_{\text{peak}}$ is the peak value up to time $t$.

\subsubsection{Total Return}

\begin{equation}
\label{eq:total_return}
R_{\text{total}} = \frac{V_T - V_0}{V_0}
\end{equation}

\subsection{Robustness Metrics}

As defined in Section~\ref{sec:adversarial}, we evaluate:
\begin{itemize}
    \item Robustness Index: $R_{\text{index}}$
    \item Adversarial Accuracy: $\text{Acc}_{\text{adv}}$
    \item Attack Resistance: Percentage reduction in performance degradation
\end{itemize}

% ============================================
% CHAPTER 8: THEORETICAL GUARANTEES
% ============================================
\section{Theoretical Guarantees and Bounds}
\label{sec:guarantees}

\subsection{Convergence Guarantees}

\begin{theorem}[MHA-DQN Convergence]
\label{thm:mha_dqn_convergence}
Under standard conditions (bounded rewards, Lipschitz Q-function, sufficient exploration), the MHA-DQN algorithm converges to a stationary policy with probability 1.
\end{theorem}

\begin{proof}
The proof combines:
\begin{enumerate}
    \item Convergence of Q-learning with function approximation (Theorem~\ref{thm:convergence})
    \item Universal approximation property of attention mechanisms
    \item Stability of residual connections (Lemma~\ref{lem:residual})
\end{enumerate}
The multi-head attention architecture maintains the contraction property required for convergence, while the adversarial training component introduces bounded perturbations that preserve convergence properties.
\end{proof}

\subsection{Robustness Bounds}

\begin{theorem}[Adversarial Robustness Bound]
\label{thm:robust_bound}
For a model trained with adversarial training using FGSM with budget $\epsilon$, the performance degradation under adversarial attack is bounded by:
\begin{equation}
\label{eq:bound}
|\text{Performance}(x) - \text{Performance}(x_{\text{adv}})| \leq C \cdot \epsilon \cdot \|\nabla_x \mathcal{L}\|_1
\end{equation}
for some constant $C$ depending on the model architecture.
\end{theorem}

\begin{proof}
By Lipschitz continuity of the Q-function and the definition of FGSM attack, we have:
\begin{align}
|Q(x) - Q(x_{\text{adv}})| &\leq L \|x - x_{\text{adv}}\|_\infty \\
&= L \epsilon \|\sign(\nabla_x \mathcal{L})\|_\infty \\
&\leq L \epsilon
\end{align}
where $L$ is the Lipschitz constant. The performance bound follows from the relationship between Q-values and portfolio performance.
\end{proof}

\subsection{Generalization Bounds}

\begin{theorem}[Generalization Bound]
\label{thm:generalization}
With probability at least $1-\delta$, the generalization error is bounded by:
\begin{equation}
\label{eq:gen_bound}
\mathcal{L}_{\text{test}} \leq \mathcal{L}_{\text{train}} + O\left(\sqrt{\frac{\log(1/\delta) + \text{VC-dim}(\mathcal{H})}{n}}\right)
\end{equation}
where $\mathcal{H}$ is the hypothesis class of MHA-DQN models and $n$ is the number of training samples.
\end{theorem}

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem{hornik1991approximation}
Hornik, K., Stinchcombe, M., \& White, H. (1991). Multilayer feedforward networks are universal approximators. \textit{Neural networks}, 2(5), 359-366.

\bibitem{tsitsiklis1997analysis}
Tsitsiklis, J. N. (1997). Asynchronous stochastic approximation and Q-learning. \textit{Machine learning}, 22(1-3), 185-202.

\bibitem{vaswani2017attention}
Vaswani, A., et al. (2017). Attention is all you need. \textit{Advances in neural information processing systems}, 30.

\bibitem{mnih2015human}
Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, 518(7540), 529-533.

\bibitem{goodfellow2014generative}
Goodfellow, I. J., Shlens, J., \& Szegedy, C. (2014). Explaining and harnessing adversarial examples. \textit{arXiv preprint arXiv:1412.6572}.

\bibitem{madry2017towards}
Madry, A., et al. (2017). Towards deep learning models resistant to adversarial attacks. \textit{arXiv preprint arXiv:1706.06083}.

\bibitem{carlini2017towards}
Carlini, N., \& Wagner, D. (2017). Towards evaluating the robustness of neural networks. \textit{2017 IEEE symposium on security and privacy (sp)} (pp. 39-57). IEEE.

\bibitem{moosavi2016deepfool}
Moosavi-Dezfooli, S. M., Fawzi, A., \& Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. \textit{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 2574-2582).

\bibitem{wang2016dueling}
Wang, Z., et al. (2016). Dueling network architectures for deep reinforcement learning. \textit{International conference on machine learning} (pp. 1995-2003). PMLR.

\bibitem{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., \& Silver, D. (2015). Prioritized experience replay. \textit{arXiv preprint arXiv:1511.05952}.

\end{thebibliography}

\end{document}

